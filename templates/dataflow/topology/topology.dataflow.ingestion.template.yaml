AWSTemplateFormatVersion: '2010-09-09'
Transform: 'AWS::Serverless-2016-10-31'
Description: "Deploys raw streaming ingestion pipeline for grid topology data. (qs-1r18anahd)"

Parameters:
  RawDataBucket:
    Type: String

  QSS3KeyPrefix:
    AllowedPattern: '^[0-9a-zA-Z-/]*[/]$'
    ConstraintDescription: >-
      Quick Start key prefix can include numbers, lowercase letters, uppercase
      letters, hyphens (-), and forward slash (/) and must terminate in a forward slash.
    Default: quickstart-aws-utility-meter-data-analytics-platform/
    Type: String
    Description: S3 key prefix for the Quick Start assets. Quick Start key prefix
      can include numbers, lowercase letters, uppercase letters, hyphens (-), and
      forward slash (/).

  QSS3BucketName:
    AllowedPattern: '^[0-9a-zA-Z]+([0-9a-zA-Z-]*[0-9a-zA-Z])*$'
    ConstraintDescription: >-
      Quick Start bucket name can include numbers, lowercase letters, uppercase
      letters, and hyphens (-). It cannot start or end with a hyphen (-).
    Default: aws-quickstart
    Description: >-
      S3 bucket name for the Quick Start assets.
      Only change this value if you customize or extend the Quick Start for your own use.
      This string can include numbers, lowercase letters, uppercase letters, and hyphens (-).
      It cannot start or end with a hyphen (-).
    Type: String
  QSS3BucketRegion:
    Default: 'us-east-1'
    Description: 'AWS Region where the Quick Start S3 bucket (QSS3BucketName) is 
      hosted. Keep the default Region unless you are customizing the template. 
      Changing this Region updates code references to point to a new Quick Start location. 
      When using your own bucket, specify the Region. 
      See https://aws-quickstart.github.io/option1.html.'
    Type: String

Conditions:
  UsingDefaultBucket: !Equals [!Ref QSS3BucketName, 'aws-quickstart']

Resources:

  #
  # Copy GridTopology showcase data, copy on deployment
  # Initial load fires an event to trigger integration pipeline
  #

  CopyGridTopologyArtifacts:
    Type: Custom::CopyArtifacts
    Properties:
      ServiceToken: !GetAtt 'CopyGridTopologyFunction.Arn'
      DestBucket: !Ref RawDataBucket
      DestPrefix: 'topology/'
      SourceBucket: !Ref QSS3BucketName
      Prefix: !Sub '${QSS3KeyPrefix}showcase/data/grid_topology/'
      Objects:
        - distribution_access_point.json
        - distribution_point.json
        - location.json
        - power_grid.json
        - service_location.json
        - smart_meter.json
        - voltage_control_zone.json

  CopyGridTopologyRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - !Sub 'arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Path: /
      Policies:
        - PolicyName: lambda-grid-topology-copier
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${QSS3BucketName}/${QSS3KeyPrefix}*'
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:ListBucket
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${RawDataBucket}/*'
                  - !Sub 'arn:${AWS::Partition}:s3:::${RawDataBucket}'
              - Effect: Allow
                Action:
                  - events:PutEvents
                Resource:
                  - !Sub 'arn:${AWS::Partition}:events:${AWS::Region}:${AWS::AccountId}:event-bus/default'


  CopyGridTopologyFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Copies GridTopology data from a source S3 bucket to a destination
      Handler: index.handler
      Runtime: python3.9
      Role: !GetAtt CopyGridTopologyRole.Arn
      Timeout: 240
      Code:
        ZipFile: |
          import json
          import logging
          import threading
          import boto3
          import cfnresponse
          logging.getLogger().setLevel(logging.INFO)
          
          s3 = boto3.client('s3')
          events = boto3.client('events')
          def copy_objects(source_bucket, dest_bucket, prefix, dest_prefix, objects):
              for o in objects:
                  key = prefix + o
                  copy_source = {
                      'Bucket': source_bucket,
                      'Key': key
                  }
                  # use filename as subfolder to separate the different grid components
                  sub_folder = o.replace(".json","")
                  dest_key = f"{dest_prefix}{sub_folder}/{o}"
                  
                  logging.info('copy_source: %s' % copy_source)
                  logging.info('dest_bucket = %s'%dest_bucket)
                  logging.info('key = %s' %key)
                  logging.info('dest_key = %s' % dest_key)
                  s3.copy_object(CopySource=copy_source, Bucket=dest_bucket,
                        Key=dest_key)
          def timeout(event, context):
              logging.error('Execution is about to time out, sending failure response to CloudFormation')
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None)
          def send_event(dest_bucket, dest_prefix, objects):
              keys = []
              for o in objects:
                sub_folder = o.replace(".json","")
                dest_key = f"{dest_prefix}{sub_folder}/{o}"
                keys.append(dest_key)
              
              detail =  {
                'bucket': dest_bucket,
                'objects': json.dumps(keys)
              }
              logging.info(detail)
              events.put_events(
                  Entries=[
                      {
                    'Source': 'mda.topology',
                    'Resources': [
                      f"arn:aws:s3:::{dest_bucket}"
                    ],
                    'DetailType': 'New Topology Data',
                    'Detail': json.dumps(detail)
                  }
                      ]
                )
          
          def handler(event, context):
              logging.info('Received event: %s' % json.dumps(event))
              # make sure we send a failure to CloudFormation if the function
              # is going to timeout
              timer = threading.Timer((context.get_remaining_time_in_millis()
                        / 1000.00) - 0.5, timeout, args=[event, context])
              timer.start()
              status = cfnresponse.SUCCESS
              try:
                  source_bucket = event['ResourceProperties']['SourceBucket']
                  dest_bucket = event['ResourceProperties']['DestBucket']
                  dest_prefix = event['ResourceProperties']['DestPrefix']
                  prefix = event['ResourceProperties']['Prefix']
                  objects = event['ResourceProperties']['Objects']
                  if event['RequestType'] == 'Delete':
                      logging.info(f"{dest_bucket} will be cleaned by datastorage stack.")
                  else:
                      copy_objects(source_bucket, dest_bucket, prefix, dest_prefix, objects)
                      send_event(dest_bucket, dest_prefix, objects)
          
              except Exception as e:
                  logging.error('Exception: %s' % e, exc_info=True)
                  status = cfnresponse.FAILED
              finally:
                  timer.cancel()
                  cfnresponse.send(event, context, status, {}, None)