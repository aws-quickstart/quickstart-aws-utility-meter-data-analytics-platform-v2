// Add steps as necessary for accessing the software, post-configuration, and testing. Donâ€™t include full usage instructions for your software, but add links to your product documentation for that information.
//Should any sections not be applicable, remove them
:xrefstyle: short


== Post deployment steps

To see the capabilities of the QuickStart, follow the steps in this section to

- adjust the data generator and HES simulator to simulate different meter populations
- visualize data, forecasts, and anomalies
- use APIs to query information
- customize the Input Adapter
- customize the Dataflows (weather topology)
- customize the ETL scripts

=== Update meter generator and HES simulator settings


=== Visualization
The QuickStart provides a Grafana dashboard for visualisation, to setup the dashboard follow the following steps. TODO link to Grafana doc

=== API
TODO describe APIs

=== Input Adapter
The input adapter loads the meter reads from the external source (f.e. HES or FTP), and prepares them for the staging area. The input adapter can be exchanged to adopt different source systems and formats. This section will describe the input adapter that will be delivered with the QuickStart and explains which are the minimum changes to connect different systems and formats.

*Connector to the HES simulator, which will be deployed with the QuickStart*

.input adapter
image::../images/architecture/input_adapter.svg[Architecture,width=80%,height=80%]

The input adapter implementation connects to the provided HES simulator. The different steps are explained below, depending on the source system in question these need to be adapted.

(1) The state machine orchestrates the generation and download of the meter reads file from the HES. As soon as the meter read file has been generated it needs to be downloaded a compressed file to the inbound bucket, afterwards another process extracts the file and stores it in the uncompressed folder.

(2) The inbound bucket holds the compressed and uncompressed files, uncompressed files will be deleted and compressed files achieved to save storage and costs.

(3) As soon as the file is extracted, an event will be sent which triggers an AWS Lambda function for further processing.

(4) The File Range Extractor extracts range information (a range is a group of lines which should be processed together) from the uncompressed file based on the file size and number of chunks (configurable). Each range information will be sent to SQS.

(5) Each worker takes a range from the queue and processes the respective meter reads (parse and transform) before sending each element to Amazon Kinesis. This process ensures that the content input file can be processed in parallel. The worker transform the CSV line into JSON, and creates a separate object for each reading type.

(6) The Amazon Kinesis data stream is used to ingest the data into the staging area. The stream scales on-demand.

*SMETS*
TBD

=== Dataflows
Every external datasource is implemented as a dataflow, the dataflow connects to the external source loads the necessary data and stores them in a purpose built database from where they can be accessed through the central data catalog.

The QuickStart comes with two example dataflows for weather and topology data. To add a new dataflow, a data pipeline that loads the data from the source, prepares them and stores the results in an appropriate data store needs to be designed. Once implemented the data store needs to be added to the central data catalog from where the subsequent processes can access the data.

The architecture shows an example implementation, services can change depending on the requirements.

.custom dataflow
image::../images/architecture/custom_dataflow.svg[Architecture,width=80%,height=80%]

=== ETL Scripts





